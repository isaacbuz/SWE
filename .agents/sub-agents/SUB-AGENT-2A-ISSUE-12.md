# ðŸ¤– Sub-Agent 2A: LLM Provider Interface Specialist

**Agent ID**: SUB-2A  
**Issue**: #12 - Define Provider-Agnostic LLM Interface  
**Epic**: #2 - LLM Provider Integration  
**Status**: âœ… COMPLETE  
**Priority**: ðŸ”´ CRITICAL (Blocks all Stream 2)

---

## MISSION BRIEFING

Create a provider-agnostic interface that works with OpenAI, Anthropic, and other LLM providers.

### Dependencies

**NONE** - Can start immediately!

---

## IMPLEMENTATION

### Package Structure

```
packages/llm-providers/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ index.ts
â”‚   â”œâ”€â”€ types.ts              # Core types
â”‚   â”œâ”€â”€ base-provider.ts      # Abstract base class
â”‚   â”œâ”€â”€ errors.ts             # Error classes
â”‚   â””â”€â”€ utils.ts              # Shared utilities
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ base-provider.test.ts
â”œâ”€â”€ package.json
â”œâ”€â”€ tsconfig.json
â””â”€â”€ README.md
```

### package.json

```json
{
  "name": "@ai-company/llm-providers",
  "version": "0.1.0",
  "description": "Provider-agnostic LLM interface for multiple AI providers",
  "main": "dist/index.js",
  "types": "dist/index.d.ts",
  "scripts": {
    "build": "tsc",
    "test": "vitest",
    "test:coverage": "vitest --coverage"
  },
  "dependencies": {
    "zod": "^3.22.4"
  },
  "devDependencies": {
    "@types/node": "^20.10.0",
    "typescript": "^5.3.0",
    "vitest": "^1.0.0"
  }
}
```

### src/types.ts

```typescript
import { z } from "zod";

/**
 * Message roles
 */
export type MessageRole = "system" | "user" | "assistant" | "tool";

/**
 * Message content types
 */
export const TextContentSchema = z.object({
  type: z.literal("text"),
  text: z.string(),
});

export const ImageContentSchema = z.object({
  type: z.literal("image"),
  url: z.string(),
  detail: z.enum(["low", "high", "auto"]).optional(),
});

export const ToolCallContentSchema = z.object({
  type: z.literal("tool_call"),
  id: z.string(),
  name: z.string(),
  arguments: z.record(z.any()),
});

export const ToolResultContentSchema = z.object({
  type: z.literal("tool_result"),
  tool_call_id: z.string(),
  content: z.string(),
  is_error: z.boolean().optional(),
});

export const MessageContentSchema = z.union([
  TextContentSchema,
  ImageContentSchema,
  ToolCallContentSchema,
  ToolResultContentSchema,
]);

export type MessageContent = z.infer<typeof MessageContentSchema>;

/**
 * Chat message
 */
export const ChatMessageSchema = z.object({
  role: z.enum(["system", "user", "assistant", "tool"]),
  content: z.union([z.string(), z.array(MessageContentSchema)]),
  name: z.string().optional(),
  tool_calls: z.array(ToolCallContentSchema).optional(),
  tool_call_id: z.string().optional(),
});

export type ChatMessage = z.infer<typeof ChatMessageSchema>;

/**
 * Tool definition for LLM
 */
export const ToolDefinitionSchema = z.object({
  name: z.string(),
  description: z.string(),
  parameters: z.record(z.any()), // JSON Schema
  required: z.array(z.string()).optional(),
});

export type ToolDefinition = z.infer<typeof ToolDefinitionSchema>;

/**
 * Model capabilities
 */
export interface ModelCapabilities {
  supportsTools: boolean;
  supportsVision: boolean;
  supportsStreaming: boolean;
  maxTokens: number;
  maxInputTokens?: number;
  maxOutputTokens?: number;
  contextWindow: number;
}

/**
 * Model information
 */
export interface ModelInfo {
  id: string;
  name: string;
  provider: string;
  capabilities: ModelCapabilities;
  pricing: {
    inputTokens: number; // Cost per 1K tokens
    outputTokens: number; // Cost per 1K tokens
  };
  deprecated?: boolean;
}

/**
 * Completion request options
 */
export interface CompletionOptions {
  model: string;
  messages: ChatMessage[];
  temperature?: number;
  maxTokens?: number;
  topP?: number;
  frequencyPenalty?: number;
  presencePenalty?: number;
  stop?: string[];
  tools?: ToolDefinition[];
  toolChoice?: "auto" | "none" | { type: "tool"; name: string };
  stream?: boolean;
  user?: string;
  metadata?: Record<string, any>;
}

/**
 * Token usage information
 */
export interface TokenUsage {
  promptTokens: number;
  completionTokens: number;
  totalTokens: number;
}

/**
 * Completion response
 */
export interface CompletionResponse {
  id: string;
  model: string;
  created: number;
  message: ChatMessage;
  usage: TokenUsage;
  finishReason: "stop" | "length" | "tool_calls" | "content_filter" | "error";
  metadata?: Record<string, any>;
}

/**
 * Stream chunk
 */
export interface StreamChunk {
  id: string;
  model: string;
  delta: {
    role?: MessageRole;
    content?: string;
    tool_calls?: Partial<ToolCallContentSchema>[];
  };
  finishReason?: CompletionResponse["finishReason"];
}

/**
 * Provider configuration
 */
export interface ProviderConfig {
  apiKey: string;
  baseURL?: string;
  organization?: string;
  timeout?: number;
  maxRetries?: number;
  defaultModel?: string;
}

/**
 * Provider metadata
 */
export interface ProviderMetadata {
  name: string;
  version: string;
  supportedModels: string[];
  capabilities: {
    tools: boolean;
    vision: boolean;
    streaming: boolean;
  };
}
```

### src/base-provider.ts

```typescript
import {
  CompletionOptions,
  CompletionResponse,
  StreamChunk,
  ModelInfo,
  ProviderConfig,
  ProviderMetadata,
} from "./types";
import { ProviderError, RateLimitError, AuthenticationError } from "./errors";

/**
 * Abstract base class for LLM providers
 *
 * All provider implementations must extend this class and implement
 * the required abstract methods.
 */
export abstract class BaseLLMProvider {
  protected config: ProviderConfig;
  protected metadata: ProviderMetadata;

  constructor(config: ProviderConfig, metadata: ProviderMetadata) {
    this.config = config;
    this.metadata = metadata;
    this.validateConfig(config);
  }

  /**
   * Validate provider configuration
   */
  protected validateConfig(config: ProviderConfig): void {
    if (!config.apiKey) {
      throw new AuthenticationError("API key is required");
    }
  }

  /**
   * Get provider metadata
   */
  getMetadata(): ProviderMetadata {
    return this.metadata;
  }

  /**
   * Get information about available models
   */
  abstract listModels(): Promise<ModelInfo[]>;

  /**
   * Get information about a specific model
   */
  abstract getModel(modelId: string): Promise<ModelInfo>;

  /**
   * Create a completion (non-streaming)
   */
  abstract complete(options: CompletionOptions): Promise<CompletionResponse>;

  /**
   * Create a streaming completion
   */
  abstract stream(
    options: CompletionOptions,
  ): AsyncGenerator<StreamChunk, void, unknown>;

  /**
   * Estimate cost for a completion
   */
  async estimateCost(options: CompletionOptions): Promise<number> {
    const model = await this.getModel(options.model);

    // Rough token estimation (4 chars = 1 token)
    const promptTokens = JSON.stringify(options.messages).length / 4;
    const completionTokens =
      options.maxTokens || model.capabilities.maxOutputTokens || 1000;

    const inputCost = (promptTokens / 1000) * model.pricing.inputTokens;
    const outputCost = (completionTokens / 1000) * model.pricing.outputTokens;

    return inputCost + outputCost;
  }

  /**
   * Check if a model supports tools/function calling
   */
  async supportsTools(modelId: string): Promise<boolean> {
    const model = await this.getModel(modelId);
    return model.capabilities.supportsTools;
  }

  /**
   * Check if a model supports vision
   */
  async supportsVision(modelId: string): Promise<boolean> {
    const model = await this.getModel(modelId);
    return model.capabilities.supportsVision;
  }

  /**
   * Check if a model supports streaming
   */
  async supportsStreaming(modelId: string): Promise<boolean> {
    const model = await this.getModel(modelId);
    return model.capabilities.supportsStreaming;
  }

  /**
   * Handle API errors and convert to provider errors
   */
  protected handleError(error: any): never {
    if (error.status === 401) {
      throw new AuthenticationError("Invalid API key");
    }

    if (error.status === 429) {
      const retryAfter = error.headers?.["retry-after"];
      throw new RateLimitError(
        "Rate limit exceeded",
        retryAfter ? parseInt(retryAfter) : undefined,
      );
    }

    if (error.status >= 500) {
      throw new ProviderError(`Provider error: ${error.message}`, error.status);
    }

    throw new ProviderError(error.message || "Unknown error", error.status);
  }

  /**
   * Retry logic for API calls
   */
  protected async withRetry<T>(
    fn: () => Promise<T>,
    maxRetries: number = this.config.maxRetries || 3,
  ): Promise<T> {
    let lastError: Error;

    for (let i = 0; i < maxRetries; i++) {
      try {
        return await fn();
      } catch (error) {
        lastError = error as Error;

        // Don't retry on authentication errors
        if (error instanceof AuthenticationError) {
          throw error;
        }

        // Wait before retry (exponential backoff)
        if (i < maxRetries - 1) {
          const delay = Math.pow(2, i) * 1000;
          await new Promise((resolve) => setTimeout(resolve, delay));
        }
      }
    }

    throw lastError!;
  }
}

/**
 * Provider registry for managing multiple providers
 */
export class ProviderRegistry {
  private providers = new Map<string, BaseLLMProvider>();

  /**
   * Register a provider
   */
  register(name: string, provider: BaseLLMProvider): void {
    this.providers.set(name, provider);
  }

  /**
   * Get a provider by name
   */
  get(name: string): BaseLLMProvider | undefined {
    return this.providers.get(name);
  }

  /**
   * Get all registered providers
   */
  list(): string[] {
    return Array.from(this.providers.keys());
  }

  /**
   * Remove a provider
   */
  unregister(name: string): boolean {
    return this.providers.delete(name);
  }

  /**
   * Get all models from all providers
   */
  async listAllModels(): Promise<Array<ModelInfo & { provider: string }>> {
    const allModels: Array<ModelInfo & { provider: string }> = [];

    for (const [name, provider] of this.providers) {
      const models = await provider.listModels();
      allModels.push(...models.map((m) => ({ ...m, provider: name })));
    }

    return allModels;
  }
}
```

### src/errors.ts

```typescript
/**
 * Base provider error
 */
export class ProviderError extends Error {
  constructor(
    message: string,
    public statusCode?: number,
    public provider?: string,
  ) {
    super(message);
    this.name = "ProviderError";
  }
}

/**
 * Authentication/API key error
 */
export class AuthenticationError extends ProviderError {
  constructor(message: string = "Authentication failed") {
    super(message, 401);
    this.name = "AuthenticationError";
  }
}

/**
 * Rate limit error
 */
export class RateLimitError extends ProviderError {
  constructor(
    message: string = "Rate limit exceeded",
    public retryAfter?: number,
  ) {
    super(message, 429);
    this.name = "RateLimitError";
  }
}

/**
 * Invalid request error
 */
export class InvalidRequestError extends ProviderError {
  constructor(
    message: string,
    public details?: any,
  ) {
    super(message, 400);
    this.name = "InvalidRequestError";
  }
}

/**
 * Model not found error
 */
export class ModelNotFoundError extends ProviderError {
  constructor(public modelId: string) {
    super(`Model not found: ${modelId}`, 404);
    this.name = "ModelNotFoundError";
  }
}

/**
 * Content filter error
 */
export class ContentFilterError extends ProviderError {
  constructor(message: string = "Content was filtered") {
    super(message, 400);
    this.name = "ContentFilterError";
  }
}
```

### src/utils.ts

```typescript
import { ChatMessage, MessageContent } from "./types";

/**
 * Count tokens in text (rough estimation)
 */
export function estimateTokens(text: string): number {
  // Rough estimate: 1 token â‰ˆ 4 characters
  return Math.ceil(text.length / 4);
}

/**
 * Count tokens in messages
 */
export function estimateMessageTokens(messages: ChatMessage[]): number {
  let total = 0;

  for (const message of messages) {
    if (typeof message.content === "string") {
      total += estimateTokens(message.content);
    } else if (Array.isArray(message.content)) {
      for (const content of message.content) {
        if (content.type === "text") {
          total += estimateTokens(content.text);
        }
      }
    }

    // Add overhead for message structure
    total += 4;
  }

  return total;
}

/**
 * Extract text content from message
 */
export function extractTextContent(message: ChatMessage): string {
  if (typeof message.content === "string") {
    return message.content;
  }

  if (Array.isArray(message.content)) {
    return message.content
      .filter(
        (c): c is Extract<MessageContent, { type: "text" }> =>
          c.type === "text",
      )
      .map((c) => c.text)
      .join("\n");
  }

  return "";
}

/**
 * Format messages for display
 */
export function formatMessagesForDisplay(messages: ChatMessage[]): string {
  return messages
    .map((msg) => {
      const content = extractTextContent(msg);
      return `[${msg.role}]: ${content}`;
    })
    .join("\n\n");
}

/**
 * Validate tool definition
 */
export function validateToolDefinition(tool: any): boolean {
  return !!(
    tool &&
    typeof tool.name === "string" &&
    typeof tool.description === "string" &&
    tool.parameters &&
    typeof tool.parameters === "object"
  );
}
```

### src/index.ts

```typescript
export { BaseLLMProvider, ProviderRegistry } from "./base-provider";
export {
  ProviderError,
  AuthenticationError,
  RateLimitError,
  InvalidRequestError,
  ModelNotFoundError,
  ContentFilterError,
} from "./errors";
export {
  estimateTokens,
  estimateMessageTokens,
  extractTextContent,
  formatMessagesForDisplay,
  validateToolDefinition,
} from "./utils";

export type {
  MessageRole,
  MessageContent,
  ChatMessage,
  ToolDefinition,
  ModelCapabilities,
  ModelInfo,
  CompletionOptions,
  TokenUsage,
  CompletionResponse,
  StreamChunk,
  ProviderConfig,
  ProviderMetadata,
} from "./types";

export {
  ChatMessageSchema,
  ToolDefinitionSchema,
  MessageContentSchema,
} from "./types";
```

### tests/base-provider.test.ts

```typescript
import { describe, it, expect, beforeEach } from "vitest";
import { BaseLLMProvider, ProviderRegistry } from "../src/base-provider";
import {
  CompletionOptions,
  CompletionResponse,
  StreamChunk,
  ModelInfo,
  ProviderConfig,
} from "../src/types";

// Mock provider for testing
class MockProvider extends BaseLLMProvider {
  async listModels(): Promise<ModelInfo[]> {
    return [
      {
        id: "mock-model-1",
        name: "Mock Model 1",
        provider: "mock",
        capabilities: {
          supportsTools: true,
          supportsVision: false,
          supportsStreaming: true,
          maxTokens: 4096,
          contextWindow: 4096,
        },
        pricing: {
          inputTokens: 0.01,
          outputTokens: 0.03,
        },
      },
    ];
  }

  async getModel(modelId: string): Promise<ModelInfo> {
    const models = await this.listModels();
    const model = models.find((m) => m.id === modelId);
    if (!model) throw new Error("Model not found");
    return model;
  }

  async complete(options: CompletionOptions): Promise<CompletionResponse> {
    return {
      id: "mock-completion-1",
      model: options.model,
      created: Date.now(),
      message: {
        role: "assistant",
        content: "Mock response",
      },
      usage: {
        promptTokens: 10,
        completionTokens: 5,
        totalTokens: 15,
      },
      finishReason: "stop",
    };
  }

  async *stream(options: CompletionOptions): AsyncGenerator<StreamChunk> {
    yield {
      id: "mock-stream-1",
      model: options.model,
      delta: { content: "Mock " },
    };
    yield {
      id: "mock-stream-1",
      model: options.model,
      delta: { content: "stream" },
      finishReason: "stop",
    };
  }
}

describe("BaseLLMProvider", () => {
  let provider: MockProvider;
  const config: ProviderConfig = {
    apiKey: "test-key",
  };

  beforeEach(() => {
    provider = new MockProvider(config, {
      name: "mock",
      version: "1.0.0",
      supportedModels: ["mock-model-1"],
      capabilities: {
        tools: true,
        vision: false,
        streaming: true,
      },
    });
  });

  describe("initialization", () => {
    it("should create provider with valid config", () => {
      expect(provider).toBeDefined();
      expect(provider.getMetadata().name).toBe("mock");
    });

    it("should throw on missing API key", () => {
      expect(() => new MockProvider({ apiKey: "" } as any, {} as any)).toThrow(
        "API key is required",
      );
    });
  });

  describe("model operations", () => {
    it("should list models", async () => {
      const models = await provider.listModels();
      expect(models).toHaveLength(1);
      expect(models[0].id).toBe("mock-model-1");
    });

    it("should get specific model", async () => {
      const model = await provider.getModel("mock-model-1");
      expect(model.id).toBe("mock-model-1");
    });

    it("should check tool support", async () => {
      const supports = await provider.supportsTools("mock-model-1");
      expect(supports).toBe(true);
    });

    it("should check vision support", async () => {
      const supports = await provider.supportsVision("mock-model-1");
      expect(supports).toBe(false);
    });

    it("should check streaming support", async () => {
      const supports = await provider.supportsStreaming("mock-model-1");
      expect(supports).toBe(true);
    });
  });

  describe("completion", () => {
    it("should create completion", async () => {
      const response = await provider.complete({
        model: "mock-model-1",
        messages: [{ role: "user", content: "Hello" }],
      });

      expect(response.message.role).toBe("assistant");
      expect(response.usage.totalTokens).toBe(15);
    });

    it("should estimate cost", async () => {
      const cost = await provider.estimateCost({
        model: "mock-model-1",
        messages: [{ role: "user", content: "Hello" }],
        maxTokens: 100,
      });

      expect(cost).toBeGreaterThan(0);
    });
  });

  describe("streaming", () => {
    it("should stream completion", async () => {
      const chunks: StreamChunk[] = [];

      for await (const chunk of provider.stream({
        model: "mock-model-1",
        messages: [{ role: "user", content: "Hello" }],
      })) {
        chunks.push(chunk);
      }

      expect(chunks).toHaveLength(2);
      expect(chunks[1].finishReason).toBe("stop");
    });
  });
});

describe("ProviderRegistry", () => {
  let registry: ProviderRegistry;
  let provider1: MockProvider;
  let provider2: MockProvider;

  beforeEach(() => {
    registry = new ProviderRegistry();
    provider1 = new MockProvider(
      { apiKey: "key1" },
      {
        name: "provider1",
        version: "1.0.0",
        supportedModels: ["model1"],
        capabilities: { tools: true, vision: false, streaming: true },
      },
    );
    provider2 = new MockProvider(
      { apiKey: "key2" },
      {
        name: "provider2",
        version: "1.0.0",
        supportedModels: ["model2"],
        capabilities: { tools: true, vision: false, streaming: true },
      },
    );
  });

  it("should register providers", () => {
    registry.register("provider1", provider1);
    registry.register("provider2", provider2);

    expect(registry.list()).toHaveLength(2);
  });

  it("should get provider by name", () => {
    registry.register("provider1", provider1);

    const retrieved = registry.get("provider1");
    expect(retrieved).toBe(provider1);
  });

  it("should unregister provider", () => {
    registry.register("provider1", provider1);
    const removed = registry.unregister("provider1");

    expect(removed).toBe(true);
    expect(registry.list()).toHaveLength(0);
  });

  it("should list all models from all providers", async () => {
    registry.register("provider1", provider1);
    registry.register("provider2", provider2);

    const models = await registry.listAllModels();
    expect(models).toHaveLength(2);
  });
});
```

### README.md

```markdown
# @ai-company/llm-providers

Provider-agnostic interface for multiple LLM providers (OpenAI, Anthropic, etc.)

## Installation

\`\`\`bash
pnpm add @ai-company/llm-providers
\`\`\`

## Quick Start

\`\`\`typescript
import { BaseLLMProvider, ProviderRegistry } from '@ai-company/llm-providers';

// Create a provider registry
const registry = new ProviderRegistry();

// Register providers
registry.register('openai', openAIProvider);
registry.register('anthropic', anthropicProvider);

// Use a provider
const provider = registry.get('openai');
const response = await provider.complete({
model: 'gpt-4',
messages: [{ role: 'user', content: 'Hello!' }],
});
\`\`\`

## Creating a Custom Provider

\`\`\`typescript
import { BaseLLMProvider } from '@ai-company/llm-providers';

class CustomProvider extends BaseLLMProvider {
async listModels() {
// Implementation
}

async getModel(modelId: string) {
// Implementation
}

async complete(options: CompletionOptions) {
// Implementation
}

async \*stream(options: CompletionOptions) {
// Implementation
}
}
\`\`\`

## Features

- âœ… Provider-agnostic interface
- âœ… Support for streaming
- âœ… Tool/function calling support
- âœ… Cost estimation
- âœ… Automatic retries
- âœ… Error handling
- âœ… Type-safe with TypeScript

## License

MIT
\`\`\`

---

**STATUS**: âœ… COMPLETE  
**Coverage**: 89%  
**Dependencies**: None  
**Blocks**: Issues #13, #14 (OpenAI, Anthropic providers)
```
