# AI Provider Configuration
# Central configuration for all AI provider integrations

providers:
  anthropic:
    name: "Anthropic Claude"
    base_url: "https://api.anthropic.com"
    enabled: true
    timeout: 300
    max_retries: 3

    models:
      - id: "claude-3-5-sonnet-20241022"
        name: "Claude 3.5 Sonnet"
        context_window: 200000
        max_output_tokens: 8192
        default: true
        cost_per_1k_prompt: 0.003
        cost_per_1k_completion: 0.015

      - id: "claude-3-opus-20240229"
        name: "Claude 3 Opus"
        context_window: 200000
        max_output_tokens: 4096
        cost_per_1k_prompt: 0.015
        cost_per_1k_completion: 0.075

      - id: "claude-3-5-haiku-20241022"
        name: "Claude 3.5 Haiku"
        context_window: 200000
        max_output_tokens: 8192
        cost_per_1k_prompt: 0.001
        cost_per_1k_completion: 0.005

    rate_limits:
      requests_per_minute: 50
      requests_per_hour: 1000
      tokens_per_minute: 40000
      max_concurrent: 5

    features:
      - streaming
      - function_calling
      - vision
      - prompt_caching
      - sub_agents

    environment_variables:
      api_key: "ANTHROPIC_API_KEY"

  openai:
    name: "OpenAI GPT"
    base_url: "https://api.openai.com/v1"
    enabled: true
    timeout: 300
    max_retries: 3

    models:
      - id: "gpt-4o"
        name: "GPT-4o"
        context_window: 128000
        max_output_tokens: 16384
        default: true
        cost_per_1k_prompt: 0.0025
        cost_per_1k_completion: 0.01

      - id: "gpt-4-turbo-2024-04-09"
        name: "GPT-4 Turbo"
        context_window: 128000
        max_output_tokens: 4096
        cost_per_1k_prompt: 0.01
        cost_per_1k_completion: 0.03

      - id: "gpt-4o-mini"
        name: "GPT-4o Mini"
        context_window: 128000
        max_output_tokens: 16384
        cost_per_1k_prompt: 0.00015
        cost_per_1k_completion: 0.0006

      - id: "gpt-3.5-turbo"
        name: "GPT-3.5 Turbo"
        context_window: 16385
        max_output_tokens: 4096
        cost_per_1k_prompt: 0.0005
        cost_per_1k_completion: 0.0015

    rate_limits:
      requests_per_minute: 60
      requests_per_hour: 3000
      tokens_per_minute: 90000
      max_concurrent: 10

    features:
      - streaming
      - function_calling
      - vision
      - json_mode

    environment_variables:
      api_key: "OPENAI_API_KEY"
      organization: "OPENAI_ORG_ID"

  google:
    name: "Google Gemini"
    base_url: "https://generativelanguage.googleapis.com/v1beta"
    enabled: true
    timeout: 300
    max_retries: 3

    models:
      - id: "gemini-1.5-pro"
        name: "Gemini 1.5 Pro"
        context_window: 2000000
        max_output_tokens: 8192
        default: true
        cost_per_1k_prompt: 0.00125
        cost_per_1k_completion: 0.005

      - id: "gemini-1.5-flash"
        name: "Gemini 1.5 Flash"
        context_window: 1000000
        max_output_tokens: 8192
        cost_per_1k_prompt: 0.000075
        cost_per_1k_completion: 0.0003

      - id: "gemini-pro"
        name: "Gemini Pro"
        context_window: 32768
        max_output_tokens: 8192
        cost_per_1k_prompt: 0.0005
        cost_per_1k_completion: 0.0015

    rate_limits:
      requests_per_minute: 60
      requests_per_hour: 1000
      tokens_per_minute: 32000
      max_concurrent: 5

    features:
      - streaming
      - function_calling
      - vision
      - multimodal

    environment_variables:
      api_key: "GOOGLE_API_KEY"

  ibm:
    name: "IBM Granite"
    base_url: "https://us-south.ml.cloud.ibm.com"
    enabled: true
    timeout: 300
    max_retries: 3

    models:
      - id: "granite-20b-code-instruct"
        name: "Granite 20B Code Instruct"
        context_window: 8192
        max_output_tokens: 4096
        default: true
        cost_per_1k_prompt: 0.002
        cost_per_1k_completion: 0.008

      - id: "granite-34b-code-instruct"
        name: "Granite 34B Code Instruct"
        context_window: 8192
        max_output_tokens: 4096
        cost_per_1k_prompt: 0.003
        cost_per_1k_completion: 0.012

      - id: "granite-3b-code-base"
        name: "Granite 3B Code Base"
        context_window: 8192
        max_output_tokens: 4096
        cost_per_1k_prompt: 0.0005
        cost_per_1k_completion: 0.002

    rate_limits:
      requests_per_minute: 30
      requests_per_hour: 500
      tokens_per_minute: 20000
      max_concurrent: 5

    features:
      - streaming
      - code_execution

    environment_variables:
      api_key: "IBM_API_KEY"
      project_id: "IBM_PROJECT_ID"

  local:
    name: "Local Models"
    enabled: true
    timeout: 600
    max_retries: 3

    backends:
      ollama:
        base_url: "http://localhost:11434"
        enabled: true
        description: "Ollama local model server"
        environment_variables:
          host: "OLLAMA_HOST"

      vllm:
        base_url: "http://localhost:8000"
        enabled: false
        description: "vLLM high-performance inference"
        environment_variables:
          host: "VLLM_HOST"

    rate_limits:
      requests_per_minute: 1000
      requests_per_hour: 10000
      tokens_per_minute: 1000000
      max_concurrent: 5  # Based on hardware

    features:
      - streaming
      - zero_cost
      - privacy

# Global defaults
defaults:
  temperature: 0.7
  max_tokens: 4096
  retry_config:
    max_attempts: 3
    initial_delay: 1.0
    max_delay: 60.0
    exponential_base: 2.0
    jitter: true

  # Cost tracking
  cost_tracking:
    enabled: true
    alert_threshold: 10.0  # USD
    daily_budget: 100.0    # USD

  # Logging
  logging:
    level: "INFO"
    log_requests: true
    log_responses: false  # May contain sensitive data
    log_tokens: true
    log_costs: true

# Provider selection strategy
selection:
  strategy: "cost_optimized"  # Options: cost_optimized, performance, balanced, custom

  # Custom routing rules
  routing_rules:
    - task_type: "code_generation"
      preferred_providers: ["ibm", "openai", "anthropic"]

    - task_type: "analysis"
      preferred_providers: ["anthropic", "openai"]

    - task_type: "sensitive_data"
      preferred_providers: ["local"]
      required: true  # Force local for sensitive data

    - task_type: "long_context"
      preferred_providers: ["google", "anthropic"]

    - task_type: "vision"
      preferred_providers: ["openai", "anthropic", "google"]

# Fallback configuration
fallback:
  enabled: true
  max_fallback_attempts: 2
  fallback_order:
    - primary: "anthropic"
      fallbacks: ["openai", "google"]

    - primary: "openai"
      fallbacks: ["anthropic", "google"]

    - primary: "google"
      fallbacks: ["anthropic", "openai"]

    - primary: "ibm"
      fallbacks: ["openai", "local"]

    - primary: "local"
      fallbacks: []  # No fallback for local

# Monitoring and observability
monitoring:
  enabled: true
  metrics:
    - request_count
    - token_usage
    - cost
    - latency
    - error_rate

  alerts:
    - type: "error_rate"
      threshold: 0.1  # 10%
      window: 300     # 5 minutes

    - type: "cost"
      threshold: 50.0  # USD
      window: 3600     # 1 hour

    - type: "rate_limit"
      threshold: 0.9   # 90% of limit
