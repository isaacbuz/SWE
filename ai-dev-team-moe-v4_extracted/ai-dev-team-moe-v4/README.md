
# AI Dev Team — Mixture-of-Experts (MoE) Platform (v4)

This repository scaffolds a **platform-leveraging AI software development team** that uses a **Mixture of Experts (MoE)** router to pick the best model for each role and phase, integrates with **GitHub** for PM/PR/CI, and is designed to plug in **MCP servers** as tool backends.

- Streamlit UI to drive Plan → Design → Code → Quality → Integrate.
- Declarative **Model Registry** in `config/models.yaml` including GPT-5, Claude, Gemini 2.5, Grok 4, DeepSeek, **IBM Granite**, Qwen Coder, Llama, Codestral.
- MCP tool plan in `config/mcp_tools.yaml` + docs.
- Quality gates and GitHub automation stubs.

> Generated 2025-11-08 15:08:34 
